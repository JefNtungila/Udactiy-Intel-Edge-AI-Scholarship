Cutting a model is mostly applicable for TensorFlow models. As we saw earlier in converting these models, they sometimes have some extra complexities. Some common reasons for cutting are:

1. The model has pre- or post-processing parts that don’t translate to existing Inference Engine layers.
2. The model has a training part that is convenient to be kept in the model, but is not used during inference.
3. The model is too complex with many unsupported operations, so the complete model cannot be converted in one shot.
4. The model is one of the supported SSD models. In this case, you need to cut a post-processing part off.
5. There could be a problem with model conversion in the Model Optimizer or with inference in the Inference Engine. To localize the issue, cutting the model could help to find the problem

There’s two main command line arguments to use for cutting a model with the Model Optimizer, named intuitively as --input and --output, where they are used to feed in the layer names that should be either the new entry or exit points of the model.

### Developer Documentation
You guessed it - [【here’s】](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Cutting_Model.html) the developer documentation for cutting a model.